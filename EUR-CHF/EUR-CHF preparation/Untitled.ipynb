{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime,timedelta\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT ALL THE FILES AND KEEP ONLY DESIRERED COLUMNS OF THE EVENT DATA\n",
    "\n",
    "pd.options.mode.chained_assignment = None  #  no important. This is for avoiding the warning of chained assignments\n",
    "\n",
    "drop_cols = ['c','D','E','F','G','I','J'] \n",
    "\n",
    "AUD_USD_df = pd.read_csv('EURCHF-2000-2020-15m.csv').dropna() #AUS_USD CURRENCY DATA SET\n",
    "\n",
    "# import all the event files, variable name tells what is the event\n",
    "#use 'parse_dates' keyword ynside the 'read_csv' function in order to combine date and time also convert the data type into 'datetime64'\n",
    "AUDZ_PPI_df = pd.read_csv('AUDZ_PPI.csv',header = None,names =['DATE','TIME','c','D','E','F','G','AUD_GDP','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "AUDZ_PPI_df = AUDZ_PPI_df.drop(columns=drop_cols)\n",
    "\n",
    "CHF_CPI_df = pd.read_csv('CHF_CPI.csv',header = None,names =['DATE','TIME','c','D','E','F','G','CHF_CPI','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "CHF_CPI_df = CHF_CPI_df.drop(columns=drop_cols)\n",
    "\n",
    "CHF_GDP_df = pd.read_csv('CHF_GDP.csv',header = None,names =['DATE','TIME','c','D','E','F','G','CHF_GDP','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "CHF_GDP_df = CHF_GDP_df.drop(columns=drop_cols)\n",
    "\n",
    "CHF_IR_df = pd.read_csv('CHF_IR.csv',header = None,names =['DATE','TIME','c','D','E','F','G','CHF_IR','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "CHF_IR_df = CHF_IR_df.drop(columns=drop_cols)\n",
    "\n",
    "CHF_PPI_df = pd.read_csv('CHF_PPI.csv',header = None,names =['DATE','TIME','c','D','E','F','G','CHF_PPI','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "CHF_PPI_df = CHF_PPI_df.drop(columns=drop_cols)\n",
    "\n",
    "CHF_RETAILSALES_df = pd.read_csv('CHF_RETAILSALES.csv',header = None,names =['DATE','TIME','c','D','E','F','G','CHF_RETAILSALES','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "CHF_RETAILSALES_df = CHF_RETAILSALES_df.drop(columns=drop_cols)\n",
    "\n",
    "CHF_UNEMP_df = pd.read_csv('CHF_UNEMP.csv',header = None,names =['DATE','TIME','c','D','E','F','G','CHF_UNEMP','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "CHF_UNEMP_df = CHF_UNEMP_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDF_CPI_df = pd.read_csv('EUDF_CPI.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDF_CPI','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDF_CPI_df = EUDF_CPI_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDF_GDP_df = pd.read_csv('EUDF_GDP.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDF_GDP','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDF_GDP_df = EUDF_GDP_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDF_PAYROLL_df = pd.read_csv('EUDF_PAYROLL.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDF_PAYROLL','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDF_PAYROLL_df = EUDF_PAYROLL_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDF_PPI_df = pd.read_csv('EUDF_PPI.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDF_PPI','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDF_PPI_df = EUDF_PPI_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDF_UNEMP_df = pd.read_csv('EUDF_UNEMP.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDF_UNEMP','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDF_UNEMP_df = EUDF_UNEMP_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDG_CPI_df = pd.read_csv('EUDG_CPI.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDG_CPI','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDG_CPI_df = EUDG_CPI_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDG_GDP_df = pd.read_csv('EUDG_GDP.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDG_GDP','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDG_GDP_df = EUDG_GDP_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDG_PPI_df = pd.read_csv('EUDG_PPI.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDG_PPI','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDG_PPI_df = EUDG_PPI_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDG_RETAILSALES_df = pd.read_csv('EUDG_RETAILSALES.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDG_RETAILSALES','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDG_RETAILSALES_df = EUDG_RETAILSALES_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDG_UNEMP_df = pd.read_csv('EUDG_UNEMP.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDG_UNEMP','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDG_UNEMP_df = EUDG_UNEMP_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDI_CPI_df = pd.read_csv('EUDI_CPI.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDI_CPI','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDI_CPI_df = EUDI_CPI_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDI_GDP_df = pd.read_csv('EUDI_GDP.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDI_GDP','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDI_GDP_df = EUDI_GDP_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDI_PPI_df = pd.read_csv('EUDI_PPI.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDI_PPI','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDI_PPI_df = EUDI_PPI_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDI_RETAILSALES_df = pd.read_csv('EUDI_RETAILSALES.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDI_RETAILSALES','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDI_RETAILSALES_df = EUDI_RETAILSALES_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDS_CPI_df = pd.read_csv('EUDS_CPI.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDS_CPI','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDS_CPI_df = EUDS_CPI_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDS_GDP_df = pd.read_csv('EUDS_GDP.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDS_GDP','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDS_GDP_df = EUDS_GDP_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDS_PPI_df = pd.read_csv('EUDS_PPI.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDS_PPI','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDS_PPI_df = EUDS_PPI_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDS_RETAILSALES_df = pd.read_csv('EUDS_RETAILSALES.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDS_RETAILSALES','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDS_RETAILSALES_df = EUDS_RETAILSALES_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDS_UNEMP_df = pd.read_csv('EUDS_UNEMP.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDS_UNEMP','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDS_UNEMP_df = EUDS_UNEMP_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDZ_CPI_df = pd.read_csv('EUDZ_CPI.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDZ_CPI','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDZ_CPI_df = EUDZ_CPI_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDZ_GDP_df = pd.read_csv('EUDZ_GDP.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDZ_GDP','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDZ_GDP_df = EUDZ_GDP_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDZ_IR_df = pd.read_csv('EUDZ_IR.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDZ_IR','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDZ_IR_df = EUDZ_IR_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDZ_RETAILSALES_df = pd.read_csv('EUDZ_RETAILSALES.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDZ_RETAILSALES','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDZ_RETAILSALES_df = EUDZ_RETAILSALES_df.drop(columns=drop_cols)\n",
    "\n",
    "EUDZ_UNEMP_df = pd.read_csv('EUDZ_UNEMP.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EUDZ_UNEMP','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EUDZ_UNEMP_df = EUDZ_UNEMP_df.drop(columns=drop_cols)\n",
    "\n",
    "EURI_UNEMP_df = pd.read_csv('EURI_UNEMP.csv',header = None,names =['DATE','TIME','c','D','E','F','G','EURI_UNEMP','I','J'] ,parse_dates= [[0,1]]).dropna() \n",
    "EURI_UNEMP_df = EURI_UNEMP_df.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the data from 2011-2020 from the 'AUD_USD_df' data set where the pips are located.\n",
    "d =AUD_USD_df.loc[272555:499339 ,:]\n",
    "df = d.reset_index(drop=True)\n",
    "df['EX_DATE_TIME'] = pd.to_datetime(df['DATE_TIME']) # convert the type to datetime\n",
    "df['DATE_TIME'] = df['EX_DATE_TIME']- pd.Timedelta(hours=7)\n",
    " # substract 7 hours from the currency set\n",
    "\n",
    "CHF_UNEMP_df['DATE_TIME'] =pd.to_datetime(CHF_UNEMP_df['DATE_TIME'])\n",
    "EUDF_PAYROLL_df['DATE_TIME']= pd.to_datetime(EUDF_PAYROLL_df['DATE_TIME'])\n",
    "EUDZ_CPI_df['DATE_TIME'] = pd.to_datetime(EUDZ_CPI_df['DATE_TIME'])   \n",
    "EUDZ_GDP_df['DATE_TIME'] = pd.to_datetime(EUDZ_GDP_df['DATE_TIME']) \n",
    "EUDZ_IR_df['DATE_TIME'] = pd.to_datetime(EUDZ_IR_df['DATE_TIME'])\n",
    "EURI_UNEMP_df['DATE_TIME'] = pd.to_datetime(EURI_UNEMP_df['DATE_TIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the events details and pips details according to date\n",
    "new_df =  df.merge(AUDZ_PPI_df, on = 'DATE_TIME', how = 'left')\\\n",
    "          .merge(CHF_CPI_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(CHF_GDP_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(CHF_IR_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(CHF_PPI_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(CHF_RETAILSALES_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(CHF_UNEMP_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDF_CPI_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDF_GDP_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDF_PAYROLL_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDF_PPI_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDF_UNEMP_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDG_CPI_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDG_GDP_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDG_PPI_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDG_RETAILSALES_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDG_UNEMP_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDI_CPI_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDI_GDP_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDI_PPI_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDI_RETAILSALES_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDS_CPI_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDS_GDP_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDS_PPI_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDS_RETAILSALES_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDS_UNEMP_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDZ_CPI_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDZ_GDP_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDZ_IR_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDZ_RETAILSALES_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EUDZ_UNEMP_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(EURI_UNEMP_df, on = 'DATE_TIME', how = 'left')\n",
    "\n",
    " \n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  241  overlapping events\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE_TIME</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>LOW</th>\n",
       "      <th>OPEN</th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>EX_DATE_TIME</th>\n",
       "      <th>AUD_GDP</th>\n",
       "      <th>CHF_CPI</th>\n",
       "      <th>CHF_GDP</th>\n",
       "      <th>CHF_IR</th>\n",
       "      <th>...</th>\n",
       "      <th>EUDS_GDP</th>\n",
       "      <th>EUDS_PPI</th>\n",
       "      <th>EUDS_RETAILSALES</th>\n",
       "      <th>EUDS_UNEMP</th>\n",
       "      <th>EUDZ_CPI</th>\n",
       "      <th>EUDZ_GDP</th>\n",
       "      <th>EUDZ_IR</th>\n",
       "      <th>EUDZ_RETAILSALES</th>\n",
       "      <th>EUDZ_UNEMP</th>\n",
       "      <th>EURI_UNEMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-05 05:00:00</td>\n",
       "      <td>1.25988</td>\n",
       "      <td>1.25688</td>\n",
       "      <td>1.25906</td>\n",
       "      <td>1.25778</td>\n",
       "      <td>2011-01-05 12:00:00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-06 03:15:00</td>\n",
       "      <td>1.26923</td>\n",
       "      <td>1.26701</td>\n",
       "      <td>1.26922</td>\n",
       "      <td>1.26729</td>\n",
       "      <td>2011-01-06 10:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-06 05:00:00</td>\n",
       "      <td>1.27168</td>\n",
       "      <td>1.27086</td>\n",
       "      <td>1.27101</td>\n",
       "      <td>1.27102</td>\n",
       "      <td>2011-01-06 12:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-07 01:45:00</td>\n",
       "      <td>1.25369</td>\n",
       "      <td>1.25174</td>\n",
       "      <td>1.25331</td>\n",
       "      <td>1.25206</td>\n",
       "      <td>2011-01-07 08:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-07 02:00:00</td>\n",
       "      <td>1.25454</td>\n",
       "      <td>1.25179</td>\n",
       "      <td>1.25205</td>\n",
       "      <td>1.25389</td>\n",
       "      <td>2011-01-07 09:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>2020-03-13 04:00:00</td>\n",
       "      <td>1.05756</td>\n",
       "      <td>1.05569</td>\n",
       "      <td>1.05734</td>\n",
       "      <td>1.05601</td>\n",
       "      <td>2020-03-13 11:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535</th>\n",
       "      <td>2020-03-16 05:00:00</td>\n",
       "      <td>1.05496</td>\n",
       "      <td>1.05425</td>\n",
       "      <td>1.05484</td>\n",
       "      <td>1.05468</td>\n",
       "      <td>2020-03-16 12:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>2020-03-18 06:00:00</td>\n",
       "      <td>1.05593</td>\n",
       "      <td>1.05510</td>\n",
       "      <td>1.05539</td>\n",
       "      <td>1.05527</td>\n",
       "      <td>2020-03-18 13:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>2020-03-19 04:30:00</td>\n",
       "      <td>1.05510</td>\n",
       "      <td>1.05450</td>\n",
       "      <td>1.05469</td>\n",
       "      <td>1.05472</td>\n",
       "      <td>2020-03-19 11:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2538</th>\n",
       "      <td>2020-03-20 03:00:00</td>\n",
       "      <td>1.05554</td>\n",
       "      <td>1.05387</td>\n",
       "      <td>1.05547</td>\n",
       "      <td>1.05402</td>\n",
       "      <td>2020-03-20 10:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2539 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               DATE_TIME     HIGH      LOW     OPEN    CLOSE  \\\n",
       "0    2011-01-05 05:00:00  1.25988  1.25688  1.25906  1.25778   \n",
       "1    2011-01-06 03:15:00  1.26923  1.26701  1.26922  1.26729   \n",
       "2    2011-01-06 05:00:00  1.27168  1.27086  1.27101  1.27102   \n",
       "3    2011-01-07 01:45:00  1.25369  1.25174  1.25331  1.25206   \n",
       "4    2011-01-07 02:00:00  1.25454  1.25179  1.25205  1.25389   \n",
       "...                  ...      ...      ...      ...      ...   \n",
       "2534 2020-03-13 04:00:00  1.05756  1.05569  1.05734  1.05601   \n",
       "2535 2020-03-16 05:00:00  1.05496  1.05425  1.05484  1.05468   \n",
       "2536 2020-03-18 06:00:00  1.05593  1.05510  1.05539  1.05527   \n",
       "2537 2020-03-19 04:30:00  1.05510  1.05450  1.05469  1.05472   \n",
       "2538 2020-03-20 03:00:00  1.05554  1.05387  1.05547  1.05402   \n",
       "\n",
       "            EX_DATE_TIME  AUD_GDP  CHF_CPI  CHF_GDP  CHF_IR  ...  EUDS_GDP  \\\n",
       "0    2011-01-05 12:00:00      0.3      NaN      NaN     NaN  ...       NaN   \n",
       "1    2011-01-06 10:15:00      NaN      0.0      NaN     NaN  ...       NaN   \n",
       "2    2011-01-06 12:00:00      NaN      NaN      NaN     NaN  ...       NaN   \n",
       "3    2011-01-07 08:45:00      NaN      NaN      NaN     NaN  ...       NaN   \n",
       "4    2011-01-07 09:00:00      NaN      NaN      NaN     NaN  ...       NaN   \n",
       "...                  ...      ...      ...      ...     ...  ...       ...   \n",
       "2534 2020-03-13 11:00:00      NaN      NaN      NaN     NaN  ...       NaN   \n",
       "2535 2020-03-16 12:00:00      NaN      NaN      NaN     NaN  ...       NaN   \n",
       "2536 2020-03-18 13:00:00      NaN      NaN      NaN     NaN  ...       NaN   \n",
       "2537 2020-03-19 11:30:00      NaN      NaN      NaN   -0.75  ...       NaN   \n",
       "2538 2020-03-20 10:00:00      NaN      NaN      NaN     NaN  ...       NaN   \n",
       "\n",
       "      EUDS_PPI  EUDS_RETAILSALES  EUDS_UNEMP  EUDZ_CPI  EUDZ_GDP  EUDZ_IR  \\\n",
       "0          NaN               NaN         NaN       NaN       NaN      NaN   \n",
       "1          NaN               NaN         NaN       NaN       NaN      NaN   \n",
       "2          NaN               NaN         NaN       NaN       NaN      NaN   \n",
       "3          NaN               NaN         NaN       NaN       NaN      NaN   \n",
       "4          NaN               NaN         NaN       NaN       NaN      NaN   \n",
       "...        ...               ...         ...       ...       ...      ...   \n",
       "2534       NaN               NaN         NaN       NaN       NaN      NaN   \n",
       "2535       NaN               NaN         NaN       NaN       NaN      NaN   \n",
       "2536       NaN               NaN         NaN       1.2       NaN      NaN   \n",
       "2537       NaN               NaN         NaN       NaN       NaN      NaN   \n",
       "2538       NaN               NaN         NaN       NaN       NaN      NaN   \n",
       "\n",
       "      EUDZ_RETAILSALES  EUDZ_UNEMP  EURI_UNEMP  \n",
       "0                  NaN         NaN         NaN  \n",
       "1                  NaN         NaN         NaN  \n",
       "2                 -0.8         NaN         NaN  \n",
       "3                  NaN         NaN         NaN  \n",
       "4                  NaN         NaN         NaN  \n",
       "...                ...         ...         ...  \n",
       "2534               NaN         NaN         NaN  \n",
       "2535               NaN         NaN         NaN  \n",
       "2536               NaN         NaN         NaN  \n",
       "2537               NaN         NaN         NaN  \n",
       "2538               NaN         NaN         NaN  \n",
       "\n",
       "[2539 rows x 38 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking whether is there any overlapping events in the merged dataframe\n",
    "\n",
    "\n",
    "temp_df = new_df.drop(columns = ['DATE_TIME','HIGH','LOW','OPEN','CLOSE','EX_DATE_TIME']).notnull().sum(axis=1) # A Series of counting number of non 'nan' values accross a row\n",
    "\n",
    "#A function to checking number of overlap events\n",
    "\n",
    "def OverlapCounter(t_df):\n",
    "    count = 0\n",
    "    for i in t_df:\n",
    "        if i>1:\n",
    "            count+=1\n",
    "    print('There are ', count, ' overlapping events')\n",
    "    \n",
    "\n",
    "OverlapCounter(temp_df)# count overlap events\n",
    "\n",
    "t_df = new_df[temp_df==1] #avoid overlapping non event columns\n",
    "df_with_reset_index = t_df.reset_index(drop=True)\n",
    "df_with_reset_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE_TIME</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>LOW</th>\n",
       "      <th>OPEN</th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>Event_value</th>\n",
       "      <th>Event_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-05 05:00:00</td>\n",
       "      <td>1.25988</td>\n",
       "      <td>1.25688</td>\n",
       "      <td>1.25906</td>\n",
       "      <td>1.25778</td>\n",
       "      <td>0.30</td>\n",
       "      <td>AUD_GDP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-06 03:15:00</td>\n",
       "      <td>1.26923</td>\n",
       "      <td>1.26701</td>\n",
       "      <td>1.26922</td>\n",
       "      <td>1.26729</td>\n",
       "      <td>0.00</td>\n",
       "      <td>CHF_CPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-06 05:00:00</td>\n",
       "      <td>1.27168</td>\n",
       "      <td>1.27086</td>\n",
       "      <td>1.27101</td>\n",
       "      <td>1.27102</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>EUDZ_RETAILSALES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-07 01:45:00</td>\n",
       "      <td>1.25369</td>\n",
       "      <td>1.25174</td>\n",
       "      <td>1.25331</td>\n",
       "      <td>1.25206</td>\n",
       "      <td>3.60</td>\n",
       "      <td>CHF_UNEMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-07 02:00:00</td>\n",
       "      <td>1.25454</td>\n",
       "      <td>1.25179</td>\n",
       "      <td>1.25205</td>\n",
       "      <td>1.25389</td>\n",
       "      <td>-2.40</td>\n",
       "      <td>EUDG_RETAILSALES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>2020-03-13 04:00:00</td>\n",
       "      <td>1.05756</td>\n",
       "      <td>1.05569</td>\n",
       "      <td>1.05734</td>\n",
       "      <td>1.05601</td>\n",
       "      <td>0.70</td>\n",
       "      <td>EUDS_CPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535</th>\n",
       "      <td>2020-03-16 05:00:00</td>\n",
       "      <td>1.05496</td>\n",
       "      <td>1.05425</td>\n",
       "      <td>1.05484</td>\n",
       "      <td>1.05468</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>EUDI_CPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>2020-03-18 06:00:00</td>\n",
       "      <td>1.05593</td>\n",
       "      <td>1.05510</td>\n",
       "      <td>1.05539</td>\n",
       "      <td>1.05527</td>\n",
       "      <td>1.20</td>\n",
       "      <td>EUDZ_CPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>2020-03-19 04:30:00</td>\n",
       "      <td>1.05510</td>\n",
       "      <td>1.05450</td>\n",
       "      <td>1.05469</td>\n",
       "      <td>1.05472</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>CHF_IR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2538</th>\n",
       "      <td>2020-03-20 03:00:00</td>\n",
       "      <td>1.05554</td>\n",
       "      <td>1.05387</td>\n",
       "      <td>1.05547</td>\n",
       "      <td>1.05402</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>EUDG_PPI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2539 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               DATE_TIME     HIGH      LOW     OPEN    CLOSE  Event_value  \\\n",
       "0    2011-01-05 05:00:00  1.25988  1.25688  1.25906  1.25778         0.30   \n",
       "1    2011-01-06 03:15:00  1.26923  1.26701  1.26922  1.26729         0.00   \n",
       "2    2011-01-06 05:00:00  1.27168  1.27086  1.27101  1.27102        -0.80   \n",
       "3    2011-01-07 01:45:00  1.25369  1.25174  1.25331  1.25206         3.60   \n",
       "4    2011-01-07 02:00:00  1.25454  1.25179  1.25205  1.25389        -2.40   \n",
       "...                  ...      ...      ...      ...      ...          ...   \n",
       "2534 2020-03-13 04:00:00  1.05756  1.05569  1.05734  1.05601         0.70   \n",
       "2535 2020-03-16 05:00:00  1.05496  1.05425  1.05484  1.05468        -0.10   \n",
       "2536 2020-03-18 06:00:00  1.05593  1.05510  1.05539  1.05527         1.20   \n",
       "2537 2020-03-19 04:30:00  1.05510  1.05450  1.05469  1.05472        -0.75   \n",
       "2538 2020-03-20 03:00:00  1.05554  1.05387  1.05547  1.05402        -0.40   \n",
       "\n",
       "            Event_type  \n",
       "0              AUD_GDP  \n",
       "1              CHF_CPI  \n",
       "2     EUDZ_RETAILSALES  \n",
       "3            CHF_UNEMP  \n",
       "4     EUDG_RETAILSALES  \n",
       "...                ...  \n",
       "2534          EUDS_CPI  \n",
       "2535          EUDI_CPI  \n",
       "2536          EUDZ_CPI  \n",
       "2537            CHF_IR  \n",
       "2538          EUDG_PPI  \n",
       "\n",
       "[2539 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "x= df_with_reset_index.drop(columns = ['DATE_TIME','HIGH','LOW','OPEN','CLOSE','EX_DATE_TIME'])\n",
    "# collecting the particular event types and thier values according to the date time\n",
    "event_type = []\n",
    "event_values=[]\n",
    "for i in range(len(x)):\n",
    "    for col in x.columns:\n",
    "        val = x.loc[i,col]\n",
    "        if str(val) != 'nan':\n",
    "            event_type.append(col)\n",
    "            \n",
    "            event_values.append(val)\n",
    "            \n",
    "df_with_reset_index['Event_value'] = event_values\n",
    "df_with_reset_index['Event_type'] = event_type \n",
    "\n",
    "# rearrange the data frame by keeping only desired columns\n",
    "train_df = df_with_reset_index[['DATE_TIME','HIGH', 'LOW','OPEN','CLOSE', 'Event_value','Event_type']]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the duration\n",
    "for i in range(8):\n",
    "    \n",
    "    tempor_df = pd.DataFrame({}) # created a temporary dataframe in order to store shifting dates\n",
    "\n",
    "    tempor_df['DATE_TIME'] = train_df['DATE_TIME'] + pd.Timedelta(minutes=15*(i+1)) #shift the time by 15 minutes of slicers  \n",
    "\n",
    "    x = pd.merge(df[['DATE_TIME','HIGH','LOW','OPEN','CLOSE']],tempor_df, how='right', on='DATE_TIME') # merge the shifted date column and join HIGH, LOW values in the pip data set \n",
    "\n",
    "    x_1 = x['HIGH'].fillna(method = 'bfill') # filling 'nan' values with the next value respected to the 'nan'\n",
    "    x_2 = x['LOW'].fillna(method= 'bfill')\n",
    "    x_3 = x['CLOSE'].fillna(method = 'bfill')\n",
    "    x_4 = x['OPEN'].fillna(method = 'bfill')\n",
    "    \n",
    "    col_name = 'after_'+ str(15*(i+1)) + '_mins'#column names that included (open+close)/2 values 0-120 min in time duration\n",
    "\n",
    "    train_df[col_name] = (x_1 + x_2 + x_3 + x_4)/4\n",
    "       \n",
    "my_df = pd.DataFrame({})\n",
    " \n",
    "init_value = (train_df.iloc[:,1] + train_df.iloc[:,2] + train_df.iloc[:,3] +train_df.iloc[:,4])/4\n",
    "\n",
    "for i in range(8):\n",
    "    col = 'defference of avg from_' + str((i)*15) +'_to_' + str((i+1)*15) \n",
    "     \n",
    "     \n",
    "    my_df[col] = abs(train_df.iloc[:,i+7] - init_value)\n",
    "     \n",
    "    init_value = train_df.iloc[:,i+7]\n",
    "     \n",
    "\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[['DATE_TIME','HIGH','LOW','OPEN','CLOSE','Event_value','Event_type']]\n",
    "\n",
    "col = my_df.columns\n",
    "new_s = my_df[col].apply(lambda row: ' '.join(row.values.astype(str)), axis=1) # combine all the values in a row in the train_df into a one string\n",
    "\n",
    "def timeDuration(x):\n",
    "    a = x.split()\n",
    "    l = list(map(float,a))\n",
    "    y = ((l[0]+l[1])/2)*0.75\n",
    "    l.pop(0)\n",
    "    l.pop(0)\n",
    "    l = [x for x in l if x>=y ] \n",
    "    if len(l) >=3:\n",
    "        p = 'Long term'\n",
    "    else:\n",
    "        p= 'Short term'\n",
    "        \n",
    "    return p\n",
    "    \n",
    "train_df['Time duration'] = new_s.apply(lambda x: timeDuration(x))  \n",
    "   \n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE_TIME</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>LOW</th>\n",
       "      <th>OPEN</th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>Event_value</th>\n",
       "      <th>Event_type</th>\n",
       "      <th>Time duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-05 05:00:00</td>\n",
       "      <td>1.25988</td>\n",
       "      <td>1.25688</td>\n",
       "      <td>1.25906</td>\n",
       "      <td>1.25778</td>\n",
       "      <td>0.30</td>\n",
       "      <td>AUD_GDP</td>\n",
       "      <td>Long term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-06 03:15:00</td>\n",
       "      <td>1.26923</td>\n",
       "      <td>1.26701</td>\n",
       "      <td>1.26922</td>\n",
       "      <td>1.26729</td>\n",
       "      <td>0.00</td>\n",
       "      <td>CHF_CPI</td>\n",
       "      <td>Long term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-06 05:00:00</td>\n",
       "      <td>1.27168</td>\n",
       "      <td>1.27086</td>\n",
       "      <td>1.27101</td>\n",
       "      <td>1.27102</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>EUDZ_RETAILSALES</td>\n",
       "      <td>Short term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-07 01:45:00</td>\n",
       "      <td>1.25369</td>\n",
       "      <td>1.25174</td>\n",
       "      <td>1.25331</td>\n",
       "      <td>1.25206</td>\n",
       "      <td>3.60</td>\n",
       "      <td>CHF_UNEMP</td>\n",
       "      <td>Long term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-07 02:00:00</td>\n",
       "      <td>1.25454</td>\n",
       "      <td>1.25179</td>\n",
       "      <td>1.25205</td>\n",
       "      <td>1.25389</td>\n",
       "      <td>-2.40</td>\n",
       "      <td>EUDG_RETAILSALES</td>\n",
       "      <td>Short term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533</th>\n",
       "      <td>2020-03-13 03:45:00</td>\n",
       "      <td>1.05783</td>\n",
       "      <td>1.05702</td>\n",
       "      <td>1.05778</td>\n",
       "      <td>1.05736</td>\n",
       "      <td>0.00</td>\n",
       "      <td>EUDF_CPI</td>\n",
       "      <td>Short term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>2020-03-13 04:00:00</td>\n",
       "      <td>1.05756</td>\n",
       "      <td>1.05569</td>\n",
       "      <td>1.05734</td>\n",
       "      <td>1.05601</td>\n",
       "      <td>0.70</td>\n",
       "      <td>EUDS_CPI</td>\n",
       "      <td>Short term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535</th>\n",
       "      <td>2020-03-16 05:00:00</td>\n",
       "      <td>1.05496</td>\n",
       "      <td>1.05425</td>\n",
       "      <td>1.05484</td>\n",
       "      <td>1.05468</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>EUDI_CPI</td>\n",
       "      <td>Long term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>2020-03-18 06:00:00</td>\n",
       "      <td>1.05593</td>\n",
       "      <td>1.05510</td>\n",
       "      <td>1.05539</td>\n",
       "      <td>1.05527</td>\n",
       "      <td>1.20</td>\n",
       "      <td>EUDZ_CPI</td>\n",
       "      <td>Short term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>2020-03-19 04:30:00</td>\n",
       "      <td>1.05510</td>\n",
       "      <td>1.05450</td>\n",
       "      <td>1.05469</td>\n",
       "      <td>1.05472</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>CHF_IR</td>\n",
       "      <td>Long term</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2538 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               DATE_TIME     HIGH      LOW     OPEN    CLOSE  Event_value  \\\n",
       "0    2011-01-05 05:00:00  1.25988  1.25688  1.25906  1.25778         0.30   \n",
       "1    2011-01-06 03:15:00  1.26923  1.26701  1.26922  1.26729         0.00   \n",
       "2    2011-01-06 05:00:00  1.27168  1.27086  1.27101  1.27102        -0.80   \n",
       "3    2011-01-07 01:45:00  1.25369  1.25174  1.25331  1.25206         3.60   \n",
       "4    2011-01-07 02:00:00  1.25454  1.25179  1.25205  1.25389        -2.40   \n",
       "...                  ...      ...      ...      ...      ...          ...   \n",
       "2533 2020-03-13 03:45:00  1.05783  1.05702  1.05778  1.05736         0.00   \n",
       "2534 2020-03-13 04:00:00  1.05756  1.05569  1.05734  1.05601         0.70   \n",
       "2535 2020-03-16 05:00:00  1.05496  1.05425  1.05484  1.05468        -0.10   \n",
       "2536 2020-03-18 06:00:00  1.05593  1.05510  1.05539  1.05527         1.20   \n",
       "2537 2020-03-19 04:30:00  1.05510  1.05450  1.05469  1.05472        -0.75   \n",
       "\n",
       "            Event_type Time duration  \n",
       "0              AUD_GDP     Long term  \n",
       "1              CHF_CPI     Long term  \n",
       "2     EUDZ_RETAILSALES    Short term  \n",
       "3            CHF_UNEMP     Long term  \n",
       "4     EUDG_RETAILSALES    Short term  \n",
       "...                ...           ...  \n",
       "2533          EUDF_CPI    Short term  \n",
       "2534          EUDS_CPI    Short term  \n",
       "2535          EUDI_CPI     Long term  \n",
       "2536          EUDZ_CPI    Short term  \n",
       "2537            CHF_IR     Long term  \n",
       "\n",
       "[2538 rows x 8 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the boundary values fortrend when an event happened\n",
    "def trend(x):\n",
    "    if x == datetime.datetime(2020,3,18,20,30,0):\n",
    "        final = 0\n",
    "    else:\n",
    "        initial_value  =  x - datetime.timedelta(hours= 25)\n",
    "    \n",
    "    \n",
    "        init_check = df[df['DATE_TIME'] == initial_value]['OPEN']\n",
    "    \n",
    "    \n",
    "        forward_factor = 0\n",
    "        backward_factor = 0\n",
    "    \n",
    "        while len(init_check) != 1:\n",
    "            delta_time = datetime.timedelta(minutes=15)\n",
    "            initial_value = initial_value + delta_time\n",
    "            init_check = df[df['DATE_TIME'] == initial_value]['OPEN']\n",
    "            forward_factor+= 1\n",
    "     \n",
    "    \n",
    "        initial_value  =  x - datetime.timedelta(hours= 25)\n",
    "        init_check = df[df['DATE_TIME'] == initial_value]['OPEN']\n",
    "        while len(init_check) !=1:\n",
    "            delta_time = datetime.timedelta(minutes=15)\n",
    "            initial_value = initial_value - delta_time\n",
    "            init_check = df[df['DATE_TIME'] == initial_value]['OPEN']\n",
    "            backward_factor +=1\n",
    "     \n",
    "     \n",
    "        net_factor = backward_factor + forward_factor \n",
    "        initial_value = initial_value - datetime.timedelta(minutes=15)*(net_factor)\n",
    "     \n",
    "        max_= 0\n",
    "        min_ = 1000\n",
    "        val=0\n",
    "        for i in range(99):\n",
    "            time_delta = datetime.timedelta(minutes=15*(99-i))\n",
    "            date = x - time_delta\n",
    "            s = df[df['DATE_TIME']== date]['OPEN']\n",
    "         \n",
    "            if len(s)==1:\n",
    "                date = date\n",
    "        \n",
    "            else:\n",
    "                time_shift = datetime.timedelta(minutes=15)* (net_factor)\n",
    "                date = date - time_shift\n",
    "        \n",
    "        \n",
    "            avg_1 =(df[df['DATE_TIME'] == date ]['OPEN'] + df[df['DATE_TIME']==date ]['CLOSE'])/2   \n",
    "            avg_2 = (df[df['DATE_TIME'] == initial_value ]['OPEN'] + df[df['DATE_TIME']== initial_value ]['CLOSE'])/2     \n",
    "        \n",
    "       \n",
    "        \n",
    "            if len(avg_1) !=1  or len(avg_2) !=1:\n",
    "                val = val\n",
    "        \n",
    "            else:\n",
    "                val = abs(float(avg_1) - float(avg_2))\n",
    "         \n",
    "             \n",
    "        \n",
    "        \n",
    "            if val> max_:\n",
    "                max_=val\n",
    "            else:\n",
    "                max_ = max_\n",
    "            \n",
    "            if val < min_:\n",
    "                min_ = val\n",
    "            else:\n",
    "                min_ = min_\n",
    "        \n",
    "            initial_value = date\n",
    "     \n",
    " \n",
    "     \n",
    "  \n",
    "\n",
    "        initial_value  =  x + datetime.timedelta(hours= 25)\n",
    "    \n",
    "     \n",
    "        init_check = df[df['DATE_TIME'] == initial_value]['OPEN']\n",
    "    \n",
    "        forward_factor = 0\n",
    "        backward_factor = 0\n",
    "    \n",
    "        while len(init_check) != 1:\n",
    "            delta_time = datetime.timedelta(minutes=15)\n",
    "            initial_value = initial_value + delta_time\n",
    "            init_check = df[df['DATE_TIME'] == initial_value]['OPEN']\n",
    "            forward_factor+= 1\n",
    "        \n",
    "     \n",
    "    \n",
    "        initial_value  =  x + datetime.timedelta(hours= 25)\n",
    "        init_check = df[df['DATE_TIME'] == initial_value]['OPEN']\n",
    "    \n",
    "        while len(init_check) !=1:\n",
    "            delta_time = datetime.timedelta(minutes=15)\n",
    "            initial_value = initial_value - delta_time\n",
    "            init_check = df[df['DATE_TIME'] == initial_value]['OPEN']\n",
    "            backward_factor +=1\n",
    "        \n",
    "     \n",
    "     \n",
    "        net_factor = backward_factor + forward_factor\n",
    "        initial_value  =  x + datetime.timedelta(hours= 25) \n",
    "        initial_value = initial_value + datetime.timedelta(minutes=15)*(net_factor)\n",
    "    \n",
    "        for i in range(100):\n",
    "            time_delta = datetime.timedelta(minutes=15*(99-i))\n",
    "            date = x + time_delta\n",
    "            s = df[df['DATE_TIME']== date]['OPEN']\n",
    "        \n",
    "            if len(s)==1:\n",
    "                date = date\n",
    "        \n",
    "            else:\n",
    "                time_shift = datetime.timedelta(minutes=15)* (net_factor)\n",
    "                date = date + time_shift\n",
    "            \n",
    "                \n",
    "            avg_1 =(df[df['DATE_TIME'] == date ]['OPEN'] + df[df['DATE_TIME']==date ]['CLOSE'])/2   \n",
    "            avg_2 = (df[df['DATE_TIME'] == initial_value ]['OPEN'] + df[df['DATE_TIME']== initial_value ]['CLOSE'])/2     \n",
    "         \n",
    "            if len(avg_1) !=1  or len(avg_2) !=1 :\n",
    "                val = val\n",
    "            else:\n",
    "                 val = abs(float(avg_1) - float(avg_2))\n",
    "         \n",
    "            if val> max_:\n",
    "                max_=val\n",
    "            else:\n",
    "                max_ = max_\n",
    "            \n",
    "            if val < min_:\n",
    "                min_ = val\n",
    "            else:\n",
    "                min_ = min_\n",
    "        \n",
    "            initial_value = date\n",
    "        final = min_ + (max_ - min_)*0.10 \n",
    "        \n",
    "    return final   \n",
    "           \n",
    "    \n",
    "         \n",
    "trend(train_df['DATE_TIME'][25] )\n",
    "train_df = train_df.iloc[:2538,:]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df['Boundaries'] = train_df['DATE_TIME'].apply(lambda x : trend(x))\n",
    "for i in range(2):\n",
    "    \n",
    "    tempor_df = pd.DataFrame({}) # created a temporary dataframe in order to store shifting dates\n",
    "    \n",
    "    tempor_df['DATE_TIME'] = train_df['DATE_TIME'] - pd.Timedelta(minutes=15*(2-i)) #shift the time by 15 minutes of slicers  \n",
    "\n",
    "    x = pd.merge(df[['DATE_TIME','HIGH','LOW','OPEN','CLOSE']],tempor_df, how='right', on='DATE_TIME') # merge the shifted date column and join HIGH, LOW values in the pip data set \n",
    "\n",
    "    \n",
    "    x_1 = x['CLOSE'].fillna(method = 'bfill')\n",
    "    x_2 = x['OPEN'].fillna(method = 'bfill')\n",
    "    \n",
    "    col_name = '_before'+ str(15*(2-i)) + '_mins' + '_in_trend'#column names that included (open+close)/2 values 0-120 min in time duration\n",
    "\n",
    "    train_df[col_name] = (x_1 + x_2 )/2\n",
    "    \n",
    "for i in range(8):\n",
    "    \n",
    "    tempor_df = pd.DataFrame({}) # created a temporary dataframe in order to store shifting dates\n",
    "\n",
    "    tempor_df['DATE_TIME'] = train_df['DATE_TIME'] + pd.Timedelta(minutes=15*(1+i)) #shift the time by 15 minutes of slicers  \n",
    "\n",
    "    x = pd.merge(df[['DATE_TIME','HIGH','LOW','OPEN','CLOSE']],tempor_df, how='right', on='DATE_TIME') # merge the shifted date column and join HIGH, LOW values in the pip data set \n",
    "\n",
    "    \n",
    "    x_1 = x['CLOSE'].fillna(method = 'bfill')\n",
    "    x_2 = x['OPEN'].fillna(method = 'bfill')\n",
    "    \n",
    "    col_name = '_after'+ str(15*(1+i)) + '_mins' + '_in_trend'#column names that included (open+close)/2 values 0-120 min in time duration\n",
    "\n",
    "    train_df[col_name] = (x_1 + x_2 )/2\n",
    "    \n",
    "train_df\n",
    "my_df = pd.DataFrame({})\n",
    " \n",
    "init_value = (train_df.iloc[:,3] +train_df.iloc[:,4])/2\n",
    "\n",
    "for i in range(8):\n",
    "    col = 'defference of avg in ' +  'slot ' + str(i+1) + ' in trend'\n",
    "     \n",
    "     \n",
    "    my_df[col] = train_df.iloc[:,i+9] - init_value\n",
    "     \n",
    "    init_value = train_df.iloc[:,i+9]  \n",
    "train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df\n",
    "trend = []\n",
    "#trend calculation according to the boundaries\n",
    "def final_trend(x):\n",
    "    return {up_count: 'UP' ,down_count: 'DOWN', range_count: 'RANGE'}.get(x)\n",
    "\n",
    "for i in range(len(my_df)):\n",
    "    up_count = 0\n",
    "    down_count = 0\n",
    "    range_count = 0\n",
    "    for col in my_df.columns :\n",
    "        val = my_df.loc[i,col]\n",
    "        lower_boundary = (-1)*(train_df['Boundaries'][i])\n",
    "        upper_boundary = train_df['Boundaries'][i]\n",
    "        if val > upper_boundary:\n",
    "            up_count+=1\n",
    "        elif val< lower_boundary :\n",
    "            down_count+=1\n",
    "        else:\n",
    "            range_count+=1\n",
    "    x = max(up_count,down_count,range_count)\n",
    "    trend.append(final_trend(x))\n",
    "                 \n",
    "train_df['Trend'] = trend\n",
    "     \n",
    "train_df= train_df[['DATE_TIME','HIGH','LOW','OPEN','CLOSE','Event_value','Event_type','Time duration','Trend','Boundaries']]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the strength when trend = range\n",
    "x= train_df[train_df['Trend']== 'RANGE'].reset_index(drop=True)\n",
    "x['val']=0\n",
    "\n",
    "for i in range(11):\n",
    "    t= pd.DataFrame({})\n",
    "    t['DATE_TIME']= x['DATE_TIME']+pd.Timedelta(minutes = (-45) + (i+1)*15 )\n",
    "    j= pd.merge(df[['DATE_TIME','HIGH','LOW']],t, how='right', on='DATE_TIME')\n",
    "    x_1 = j['HIGH'].fillna(method = 'bfill')\n",
    "    x_2 = j['LOW'].fillna(method = 'bfill')\n",
    "    \n",
    "    val = (x_1 - x_2)/11 #Get the average of high-low \n",
    "    x['val'] = x['val'] + val\n",
    "\n",
    "range_dict = dict(x.groupby('Event_type').val.mean())\n",
    "print(range_dict)\n",
    "count = 0\n",
    "for i in range_dict.values():\n",
    "    range_dict[list(range_dict.keys())[count]] = i/2 # deviding the final value by 2\n",
    "    count+=1\n",
    "range_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the strength for up trend\n",
    "y= train_df[train_df['Trend']== 'UP'].reset_index(drop=True)\n",
    "y['open_close_avg'] = 0\n",
    "y['high_avg'] = 0\n",
    "for i in range(11):\n",
    "    t= pd.DataFrame({})\n",
    "    t['DATE_TIME']= y['DATE_TIME'] + pd.Timedelta(minutes = (-45) + (i+1)*15 )\n",
    "    k= pd.merge(df[['DATE_TIME','HIGH','LOW','OPEN','CLOSE']],t, how='right', on='DATE_TIME')\n",
    "    y_1 = k['CLOSE'].fillna(method = 'bfill')\n",
    "    y_2 = k['OPEN'].fillna(method = 'bfill')\n",
    "    y_3 = k['HIGH'].fillna(method = 'bfill')\n",
    "    \n",
    "    open_close_avg = (y_1 + y_2)/2\n",
    "    y['high_avg'] = y['high_avg'] +  y_3\n",
    "    y['open_close_avg'] = y['open_close_avg'] + open_close_avg\n",
    "    \n",
    "y['strength'] = (y['high_avg'])/11 - (y['open_close_avg'])/11\n",
    "y= y.set_index('DATE_TIME')\n",
    "    \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the strength for down trend\n",
    "z= train_df[train_df['Trend']== 'DOWN'].reset_index(drop=True)\n",
    "z['open_close_avg'] = 0\n",
    "z['low_avg'] = 0\n",
    "for i in range(11):\n",
    "    t= pd.DataFrame({})\n",
    "    t['DATE_TIME']= z['DATE_TIME'] + pd.Timedelta(minutes = (-45) + (i+1)*15 )\n",
    "    l= pd.merge(df[['DATE_TIME','HIGH','LOW','OPEN','CLOSE']],t, how='right', on='DATE_TIME')\n",
    "    z_1 = l['CLOSE'].fillna(method = 'bfill')\n",
    "    z_2 = l['OPEN'].fillna(method = 'bfill')\n",
    "    z_3 = l['LOW'].fillna(method = 'bfill')\n",
    "    \n",
    "    open_close_avg = (z_1 + z_2)/2\n",
    "    z['low_avg'] = z['low_avg'] +  z_3\n",
    "    z['open_close_avg'] = z['open_close_avg'] + open_close_avg\n",
    "    \n",
    "z['strength'] = (z['low_avg'])/11 - (z['open_close_avg'])/11\n",
    "z= z.set_index('DATE_TIME') \n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strength = []\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "    if train_df['Trend'][i] == 'DOWN' :\n",
    "        strength.append(z['strength'][train_df['DATE_TIME'][i]])\n",
    "    elif train_df['Trend'][i] == 'UP' :\n",
    "        strength.append(y['strength'][train_df['DATE_TIME'][i]])\n",
    "     \n",
    "    else :\n",
    "        strength.append(range_dict[train_df['Event_type'][i]])\n",
    "train_df['strength'] = strength\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['time'] = train_df['DATE_TIME'].dt.time # obtain the particular time from date time and included as a new column to the dataframe\n",
    "train_df['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('AUD-USD train set.csv', encoding = \"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('AUD-USD train set.csv', encoding = \"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
