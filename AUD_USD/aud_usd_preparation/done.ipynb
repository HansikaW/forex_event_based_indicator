{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime,timedelta\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#IMPORT ALL THE FILES AND KEEP ONLY DESIRERED COLUMNS OF THE EVENT DATA\n",
    "\n",
    "pd.options.mode.chained_assignment = None  #  no important. This is for avoiding the warning of chained assignments\n",
    "\n",
    "drop_cols = ['c','D','E','F','G','I','J'] \n",
    "\n",
    "AUD_USD_df = pd.read_csv('AUDUSD-2000-2020-15m.csv') #AUS_USD CURRENCY DATA SET\n",
    "\n",
    "# import all the event files, variable name tells what is the event\n",
    "#use 'parse_dates' keyword ynside the 'read_csv' function in order to combine date and time also convert the data type into 'datetime64'\n",
    "AUD_GDP_df = pd.read_csv('AUD_GDP.csv',header = None,names =['DATE','TIME','c','D','E','F','G','AUD_GDP','I','J'] ,parse_dates= [[0,1]]) \n",
    "AUD_GDP_df = AUD_GDP_df.drop(columns=drop_cols)\n",
    "\n",
    "\n",
    "AUD_PPI_df = pd.read_csv('AUD_PPI.csv',header = None,names = ['DATE','TIME','c','D','E','F','G','AUD_PPI','I','J'],parse_dates= [[0,1]])\n",
    "AUD_PPI_df = AUD_PPI_df.drop(columns=drop_cols)\n",
    "\n",
    "AUD_RETAILSALES_df = pd.read_csv('AUD_RETAILSALES.csv',header = None,names = ['DATE','TIME','c','D','E','F','G','AUD_RETAILSALES','I','J'],parse_dates= [[0,1]])\n",
    "AUD_RETAILSALES_df = AUD_RETAILSALES_df.drop(columns=drop_cols)\n",
    "\n",
    "AUD_UNEMP_df = pd.read_csv('AUD_UNEMP.csv',header = None,names = ['DATE','TIME','c','D','E','F','G','AUD_UNEMP','I','J'],parse_dates= [[0,1]])\n",
    "AUD_UNEMP_df = AUD_UNEMP_df.drop(columns=drop_cols)\n",
    "\n",
    "AUD_CPI_df = pd.read_csv('AUD_CPI.csv',header = None,names = ['DATE','TIME','c','D','E','F','G','AUD_CPI','I','J'],parse_dates= [[0,1]])\n",
    "AUD_CPI_df = AUD_CPI_df.drop(columns=drop_cols)\n",
    "\n",
    "USD_CPI_df = pd.read_csv('USD_CPI.csv',header = None,names = ['DATE','TIME','c','D','E','F','G','USD_CPI','I','J'],parse_dates= [[0,1]])\n",
    "USD_CPI_df = USD_CPI_df.drop(columns=drop_cols)\n",
    "\n",
    "USD_GDP_df = pd.read_csv('USD_GDP.csv',header = None,names =['DATE','TIME','c','D','E','F','G','USD_GDP','I','J'],parse_dates= [[0,1]])\n",
    "USD_GDP_df = USD_GDP_df.drop(columns=drop_cols)\n",
    "\n",
    "USD_IR_df = pd.read_csv('USD_IR.csv',header = None,names = ['DATE','TIME','c','D','E','F','G','USD_IR','I','J'],parse_dates= [[0,1]])\n",
    "USD_IR_df = USD_IR_df.drop(columns=drop_cols)\n",
    "\n",
    "USD_PAYROLL_df = pd.read_csv('USD_PAYROLL.csv',header = None,names = ['DATE','TIME','c','D','E','F','G','USD_PAYROLL','I','J'],parse_dates= [[0,1]])\n",
    "USD_PAYROLL_df = USD_PAYROLL_df.drop(columns=drop_cols)\n",
    "USD_PAYROLL_df['USD_PAYROLL'] = (USD_PAYROLL_df['USD_PAYROLL'])/1000 \n",
    "\n",
    "\n",
    "USD_PPI_df = pd.read_csv('USD_PPI.csv',header = None,names = ['DATE','TIME','c','D','E','F','G','USD_PPI','I','J'],parse_dates= [[0,1]])\n",
    "USD_PPI_df = USD_PPI_df.drop(columns=drop_cols)\n",
    "\n",
    "USD_RETAIL_df = pd.read_csv('USD_RETAIL.csv',header = None,names = ['DATE','TIME','c','D','E','F','G','USD_RETAIL','I','J'],parse_dates= [[0,1]])\n",
    "USD_RETAIL_df = USD_RETAIL_df.drop(columns=drop_cols)\n",
    "\n",
    "USD_UNEMP_df = pd.read_csv('USD_UNEMP.csv',header = None,names = ['DATE','TIME','c','D','E','F','G','USD_UNEMP','I','J'],parse_dates= [[0,1]])\n",
    "USD_UNEMP_df = USD_UNEMP_df.drop(columns=drop_cols)\n",
    "\n",
    "\n",
    "USD_PAYROLL_df['USD_PAYROLL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#select the data from 2011-2020 from the 'AUD_USD_df' data set where the pips are located.\n",
    "\n",
    "d =AUD_USD_df.loc[272142:500027 ,:]\n",
    "df = d.reset_index(drop=True)\n",
    "df['EX_DATE_TIME'] = pd.to_datetime(df['DATE_TIME']) # convert the type to datetime\n",
    "df['DATE_TIME'] = df['EX_DATE_TIME']- pd.Timedelta(hours=7)\n",
    " # substract 7 hours from the currency set\n",
    "\n",
    "#merge all the dataframes (all the events of both countries and market prices by matching with the date and time values)\n",
    " \n",
    "            \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#merging the events details and pips details according to date\n",
    "new_df =  df.merge(AUD_GDP_df, on = 'DATE_TIME', how = 'left')\\\n",
    "          .merge(AUD_PPI_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(AUD_RETAILSALES_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(AUD_UNEMP_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(AUD_CPI_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(USD_PPI_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(USD_CPI_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(USD_GDP_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(USD_IR_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(USD_PAYROLL_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(USD_RETAIL_df, on = 'DATE_TIME', how = 'left')\\\n",
    "            .merge(USD_UNEMP_df, on = 'DATE_TIME', how = 'left')\n",
    "\n",
    "new_df \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#checking whether is there any overlapping events in the merged dataframe\n",
    "\n",
    "\n",
    "temp_df = new_df.drop(columns = ['DATE_TIME','HIGH','LOW','OPEN','CLOSE','EX_DATE_TIME']).notnull().sum(axis=1) # A Series of counting number of non 'nan' values accross a row\n",
    "\n",
    "#A function to checking number of overlap events\n",
    "\n",
    "def OverlapCounter(t_df):\n",
    "    count = 0\n",
    "    for i in t_df:\n",
    "        if i>1:\n",
    "            count+=1\n",
    "    print('There are ', count, ' overlapping events')\n",
    "    \n",
    "\n",
    "OverlapCounter(temp_df)# count overlap events\n",
    "\n",
    "t_df = new_df[temp_df==1] #avoid overlapping non event columns\n",
    "df_with_reset_index = t_df.reset_index(drop=True)\n",
    "df_with_reset_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "x= df_with_reset_index.drop(columns = ['DATE_TIME','HIGH','LOW','OPEN','CLOSE','EX_DATE_TIME'])\n",
    "# collecting the particular event types and thier values according to the date time\n",
    "event_type = []\n",
    "event_values=[]\n",
    "for i in range(len(x)):\n",
    "    for col in x.columns:\n",
    "        val = x.loc[i,col]\n",
    "        if str(val) != 'nan':\n",
    "            event_type.append(col)\n",
    "            \n",
    "            event_values.append(val)\n",
    "            \n",
    "df_with_reset_index['Event_value'] = event_values\n",
    "df_with_reset_index['Event_type'] = event_type \n",
    "\n",
    "# rearrange the data frame by keeping only desired columns\n",
    "train_df = df_with_reset_index[['DATE_TIME','HIGH', 'LOW','OPEN','CLOSE', 'Event_value','Event_type']]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#calculating the trend\n",
    "for i in range(8):\n",
    "    \n",
    "    tempor_df = pd.DataFrame({}) # created a temporary dataframe in order to store shifting dates\n",
    "\n",
    "    tempor_df['DATE_TIME'] = train_df['DATE_TIME'] + pd.Timedelta(minutes=15*(i+1)) #shift the time by 15 minutes of slicers  \n",
    "\n",
    "    x = pd.merge(df[['DATE_TIME','HIGH','LOW','OPEN','CLOSE']],tempor_df, how='right', on='DATE_TIME') # merge the shifted date column and join HIGH, LOW values in the pip data set \n",
    "\n",
    "    x_1 = x['HIGH'].fillna(method = 'bfill') # filling 'nan' values with the next value respected to the 'nan'\n",
    "    x_2 = x['LOW'].fillna(method= 'bfill')\n",
    "    x_3 = x['CLOSE'].fillna(method = 'bfill')\n",
    "    x_4 = x['OPEN'].fillna(method = 'bfill')\n",
    "    \n",
    "    col_name = 'after_'+ str(15*(i+1)) + '_mins'#column names that included (open+close)/2 values 0-120 min in time duration\n",
    "\n",
    "    train_df[col_name] = (x_1 + x_2 + x_3 + x_4)/4\n",
    "       \n",
    "my_df = pd.DataFrame({})\n",
    " \n",
    "init_value = (train_df.iloc[:,1] + train_df.iloc[:,2] + train_df.iloc[:,3] +train_df.iloc[:,4])/4\n",
    "\n",
    "for i in range(8):\n",
    "    col = 'defference of avg from_' + str((i)*15) +'_to_' + str((i+1)*15) \n",
    "     \n",
    "     \n",
    "    my_df[col] = abs(train_df.iloc[:,i+7] - init_value)\n",
    "     \n",
    "    init_value = train_df.iloc[:,i+7]\n",
    "     \n",
    "\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df = train_df[['DATE_TIME','HIGH','LOW','OPEN','CLOSE','Event_value','Event_type']]\n",
    "\n",
    "col = my_df.columns\n",
    "new_s = my_df[col].apply(lambda row: ' '.join(row.values.astype(str)), axis=1) # combine all the values in a row in the train_df into a one string\n",
    "\n",
    "def timeDuration(x):\n",
    "    a = x.split()\n",
    "    l = list(map(float,a))\n",
    "    y = ((l[0]+l[1])/2)*0.75\n",
    "    l.pop(0)\n",
    "    l.pop(0)\n",
    "    l = [x for x in l if x>=y ] \n",
    "    if len(l) >=3:\n",
    "        p = 'Long term'\n",
    "    else:\n",
    "        p= 'Short term'\n",
    "        \n",
    "    return p\n",
    "    \n",
    "train_df['Time duration'] = new_s.apply(lambda x: timeDuration(x))  \n",
    "   \n",
    "train_df  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#find the boundary values fortrend when an event happened\n",
    "def trend(x):\n",
    "    if x == datetime.datetime(2020,3,18,20,30,0):\n",
    "        final = 0\n",
    "    else:\n",
    "        initial_value  =  x - datetime.timedelta(hours= 25) #25 replaced by20\n",
    "    \n",
    "    \n",
    "        init_check = df[df['DATE_TIME'] == initial_value]['OPEN']\n",
    "    \n",
    "    \n",
    "        forward_factor = 0\n",
    "        backward_factor = 0\n",
    "    \n",
    "        while len(init_check) != 1:\n",
    "            delta_time = datetime.timedelta(minutes=15)\n",
    "            initial_value = initial_value + delta_time\n",
    "            init_check = df[df['DATE_TIME'] == initial_value]['OPEN']\n",
    "            forward_factor+= 1\n",
    "     \n",
    "    \n",
    "        initial_value  =  x - datetime.timedelta(hours= 25)\n",
    "        init_check = df[df['DATE_TIME'] == initial_value]['OPEN']\n",
    "        while len(init_check) !=1:\n",
    "            delta_time = datetime.timedelta(minutes=15)\n",
    "            initial_value = initial_value - delta_time\n",
    "            init_check = df[df['DATE_TIME'] == initial_value]['OPEN']\n",
    "            backward_factor +=1\n",
    "     \n",
    "     \n",
    "        net_factor = backward_factor + forward_factor \n",
    "        initial_value = initial_value - datetime.timedelta(minutes=15)*(net_factor)\n",
    "     \n",
    "        max_= 0\n",
    "        min_ = 1000\n",
    "        for i in range(99):\n",
    "            time_delta = datetime.timedelta(minutes=15*(99-i))\n",
    "            date = x - time_delta\n",
    "            s = df[df['DATE_TIME']== date]['OPEN']\n",
    "         \n",
    "            if len(s)==1:\n",
    "                date = date\n",
    "        \n",
    "            else:\n",
    "                time_shift = datetime.timedelta(minutes=15)* (net_factor)\n",
    "                date = date - time_shift\n",
    "        \n",
    "        \n",
    "            avg_1 =(df[df['DATE_TIME'] == date ]['OPEN'] + df[df['DATE_TIME']==date ]['CLOSE'])/2   \n",
    "            avg_2 = (df[df['DATE_TIME'] == initial_value ]['OPEN'] + df[df['DATE_TIME']== initial_value ]['CLOSE'])/2     \n",
    "        \n",
    "       \n",
    "        \n",
    "            if len(avg_1) !=1  or len(avg_2) !=1:\n",
    "                val = val\n",
    "        \n",
    "            else:\n",
    "                val = abs(float(avg_1) - float(avg_2))\n",
    "         \n",
    "             \n",
    "        \n",
    "        \n",
    "            if val> max_:\n",
    "                max_=val\n",
    "            else:\n",
    "                max_ = max_\n",
    "            \n",
    "            if val < min_:\n",
    "                min_ = val\n",
    "            else:\n",
    "                min_ = min_\n",
    "        \n",
    "            initial_value = date\n",
    "     \n",
    " \n",
    "     \n",
    "  \n",
    "\n",
    "        initial_value  =  x + datetime.timedelta(hours= 25)\n",
    "    \n",
    "     \n",
    "        init_check = df[df['DATE_TIME'] == initial_value]['OPEN']\n",
    "    \n",
    "        forward_factor = 0\n",
    "        backward_factor = 0\n",
    "    \n",
    "        while len(init_check) != 1:\n",
    "            delta_time = datetime.timedelta(minutes=15)\n",
    "            initial_value = initial_value + delta_time\n",
    "            init_check = df[df['DATE_TIME'] == initial_value]['OPEN']\n",
    "            forward_factor+= 1\n",
    "        \n",
    "     \n",
    "    \n",
    "        initial_value  =  x + datetime.timedelta(hours= 25)\n",
    "        init_check = df[df['DATE_TIME'] == initial_value]['OPEN']\n",
    "    \n",
    "        while len(init_check) !=1:\n",
    "            delta_time = datetime.timedelta(minutes=15)\n",
    "            initial_value = initial_value - delta_time\n",
    "            init_check = df[df['DATE_TIME'] == initial_value]['OPEN']\n",
    "            backward_factor +=1\n",
    "        \n",
    "     \n",
    "     \n",
    "        net_factor = backward_factor + forward_factor\n",
    "        initial_value  =  x + datetime.timedelta(hours= 25) \n",
    "        initial_value = initial_value + datetime.timedelta(minutes=15)*(net_factor)\n",
    "    \n",
    "        for i in range(100):\n",
    "            time_delta = datetime.timedelta(minutes=15*(99-i))\n",
    "            date = x + time_delta\n",
    "            s = df[df['DATE_TIME']== date]['OPEN']\n",
    "        \n",
    "            if len(s)==1:\n",
    "                date = date\n",
    "        \n",
    "            else:\n",
    "                time_shift = datetime.timedelta(minutes=15)* (net_factor)\n",
    "                date = date + time_shift\n",
    "            \n",
    "                \n",
    "            avg_1 =(df[df['DATE_TIME'] == date ]['OPEN'] + df[df['DATE_TIME']==date ]['CLOSE'])/2   \n",
    "            avg_2 = (df[df['DATE_TIME'] == initial_value ]['OPEN'] + df[df['DATE_TIME']== initial_value ]['CLOSE'])/2     \n",
    "         \n",
    "            if len(avg_1) !=1  or len(avg_2) !=1 :\n",
    "                val = val\n",
    "            else:\n",
    "                 val = abs(float(avg_1) - float(avg_2))\n",
    "         \n",
    "            if val> max_:\n",
    "                max_=val\n",
    "            else:\n",
    "                max_ = max_\n",
    "            \n",
    "            if val < min_:\n",
    "                min_ = val\n",
    "            else:\n",
    "                min_ = min_\n",
    "        \n",
    "            initial_value = date\n",
    "        final = min_ + (max_ - min_)*0.10 \n",
    "        \n",
    "    return final   \n",
    "           \n",
    "    \n",
    "         \n",
    "trend(train_df['DATE_TIME'][25] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " \n",
    "train_df['Boundaries'] = train_df['DATE_TIME'].apply(lambda x : trend(x))\n",
    "for i in range(2):\n",
    "    \n",
    "    tempor_df = pd.DataFrame({}) # created a temporary dataframe in order to store shifting dates\n",
    "    \n",
    "    tempor_df['DATE_TIME'] = train_df['DATE_TIME'] - pd.Timedelta(minutes=15*(2-i)) #shift the time by 15 minutes of slicers  \n",
    "\n",
    "    x = pd.merge(df[['DATE_TIME','HIGH','LOW','OPEN','CLOSE']],tempor_df, how='right', on='DATE_TIME') # merge the shifted date column and join HIGH, LOW values in the pip data set \n",
    "\n",
    "    \n",
    "    x_1 = x['CLOSE'].fillna(method = 'bfill')\n",
    "    x_2 = x['OPEN'].fillna(method = 'bfill')\n",
    "    \n",
    "    col_name = '_before'+ str(15*(2-i)) + '_mins' + '_in_trend'#column names that included (open+close)/2 values 0-120 min in time duration\n",
    "\n",
    "    train_df[col_name] = (x_1 + x_2 )/2\n",
    "    \n",
    "for i in range(8):\n",
    "    \n",
    "    tempor_df = pd.DataFrame({}) # created a temporary dataframe in order to store shifting dates\n",
    "\n",
    "    tempor_df['DATE_TIME'] = train_df['DATE_TIME'] + pd.Timedelta(minutes=15*(1+i)) #shift the time by 15 minutes of slicers  \n",
    "\n",
    "    x = pd.merge(df[['DATE_TIME','HIGH','LOW','OPEN','CLOSE']],tempor_df, how='right', on='DATE_TIME') # merge the shifted date column and join HIGH, LOW values in the pip data set \n",
    "\n",
    "    \n",
    "    x_1 = x['CLOSE'].fillna(method = 'bfill')\n",
    "    x_2 = x['OPEN'].fillna(method = 'bfill')\n",
    "    \n",
    "    col_name = '_after'+ str(15*(1+i)) + '_mins' + '_in_trend'#column names that included (open+close)/2 values 0-120 min in time duration\n",
    "\n",
    "    train_df[col_name] = (x_1 + x_2 )/2\n",
    "    \n",
    "train_df\n",
    "my_df = pd.DataFrame({})\n",
    " \n",
    "init_value = (train_df.iloc[:,3] +train_df.iloc[:,4])/2\n",
    "\n",
    "for i in range(8):\n",
    "    col = 'defference of avg in ' +  'slot ' + str(i+1) + ' in trend'\n",
    "     \n",
    "     \n",
    "    my_df[col] = train_df.iloc[:,i+9] - init_value\n",
    "     \n",
    "    init_value = train_df.iloc[:,i+9]  \n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_df\n",
    "trend = []\n",
    "#trend calculation according to the boundaries\n",
    "def final_trend(x):\n",
    "    return {up_count: 'UP' ,down_count: 'DOWN', range_count: 'RANGE'}.get(x)\n",
    "\n",
    "for i in range(len(my_df)):\n",
    "    up_count = 0\n",
    "    down_count = 0\n",
    "    range_count = 0\n",
    "    for col in my_df.columns :\n",
    "        val = my_df.loc[i,col]\n",
    "        lower_boundary = (-1)*(train_df['Boundaries'][i])\n",
    "        upper_boundary = train_df['Boundaries'][i]\n",
    "        if val > upper_boundary:\n",
    "            up_count+=1\n",
    "        elif val< lower_boundary :\n",
    "            down_count+=1\n",
    "        else:\n",
    "            range_count+=1\n",
    "    x = max(up_count,down_count,range_count)\n",
    "    trend.append(final_trend(x))\n",
    "                 \n",
    "train_df['Trend'] = trend\n",
    "     \n",
    "train_df= train_df[['DATE_TIME','HIGH','LOW','OPEN','CLOSE','Event_value','Event_type','Time duration','Trend','Boundaries']]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#calculating the strength when trend = range\n",
    "x= train_df[train_df['Trend']== 'RANGE'].reset_index(drop=True)\n",
    "x['val']=0\n",
    "\n",
    "for i in range(11):\n",
    "    t= pd.DataFrame({})\n",
    "    t['DATE_TIME']= x['DATE_TIME']+pd.Timedelta(minutes = (-45) + (i+1)*15 )\n",
    "    j= pd.merge(df[['DATE_TIME','HIGH','LOW']],t, how='right', on='DATE_TIME')\n",
    "    x_1 = j['HIGH'].fillna(method = 'bfill')\n",
    "    x_2 = j['LOW'].fillna(method = 'bfill')\n",
    "    \n",
    "    val = (x_1 - x_2)/11 #Get the average of high-low \n",
    "    x['val'] = x['val'] + val\n",
    "\n",
    "range_dict = dict(x.groupby('Event_type').val.mean())\n",
    "print(range_dict)\n",
    "count = 0\n",
    "for i in range_dict.values():\n",
    "    range_dict[list(range_dict.keys())[count]] = i/2 # deviding the final value by 2\n",
    "    count+=1\n",
    "range_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#calculating the strength for up trend\n",
    "y= train_df[train_df['Trend']== 'UP'].reset_index(drop=True)\n",
    "y['open_close_avg'] = 0\n",
    "y['high_avg'] = 0\n",
    "for i in range(11):\n",
    "    t= pd.DataFrame({})\n",
    "    t['DATE_TIME']= y['DATE_TIME'] + pd.Timedelta(minutes = (-45) + (i+1)*15 )\n",
    "    k= pd.merge(df[['DATE_TIME','HIGH','LOW','OPEN','CLOSE']],t, how='right', on='DATE_TIME')\n",
    "    y_1 = k['CLOSE'].fillna(method = 'bfill')\n",
    "    y_2 = k['OPEN'].fillna(method = 'bfill')\n",
    "    y_3 = k['HIGH'].fillna(method = 'bfill')\n",
    "    \n",
    "    open_close_avg = (y_1 + y_2)/2\n",
    "    y['high_avg'] = y['high_avg'] +  y_3\n",
    "    y['open_close_avg'] = y['open_close_avg'] + open_close_avg\n",
    "    \n",
    "y['strength'] = (y['high_avg'])/11 - (y['open_close_avg'])/11\n",
    "y= y.set_index('DATE_TIME')\n",
    "    \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#calculating the strength for down trend\n",
    "z= train_df[train_df['Trend']== 'DOWN'].reset_index(drop=True)\n",
    "z['open_close_avg'] = 0\n",
    "z['low_avg'] = 0\n",
    "for i in range(11):\n",
    "    t= pd.DataFrame({})\n",
    "    t['DATE_TIME']= z['DATE_TIME'] + pd.Timedelta(minutes = (-45) + (i+1)*15 )\n",
    "    l= pd.merge(df[['DATE_TIME','HIGH','LOW','OPEN','CLOSE']],t, how='right', on='DATE_TIME')\n",
    "    z_1 = l['CLOSE'].fillna(method = 'bfill')\n",
    "    z_2 = l['OPEN'].fillna(method = 'bfill')\n",
    "    z_3 = l['LOW'].fillna(method = 'bfill')\n",
    "    \n",
    "    open_close_avg = (z_1 + z_2)/2\n",
    "    z['low_avg'] = z['low_avg'] +  z_3\n",
    "    z['open_close_avg'] = z['open_close_avg'] + open_close_avg\n",
    "    \n",
    "z['strength'] = (z['low_avg'])/11 - (z['open_close_avg'])/11\n",
    "z= z.set_index('DATE_TIME') \n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "strength = []\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "    if train_df['Trend'][i] == 'DOWN' :\n",
    "        strength.append(z['strength'][train_df['DATE_TIME'][i]])\n",
    "    elif train_df['Trend'][i] == 'UP' :\n",
    "        strength.append(y['strength'][train_df['DATE_TIME'][i]])\n",
    "     \n",
    "    else :\n",
    "        strength.append(range_dict[train_df['Event_type'][i]])\n",
    "train_df['strength'] = strength\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df['time'] = train_df['DATE_TIME'].dt.time # obtain the particular time from date time and included as a new column to the dataframe\n",
    "train_df['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df.to_csv('AUD-USD train set.csv', encoding = \"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
